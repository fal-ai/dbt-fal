name: Test dbt-fal encapsulate adapter

on:
  pull_request:
    types: [assigned, opened, synchronize, reopened]
    paths:
      - "adapter/**"
      - ".github/**"
      - "!.github/workflows/*cli*"

  push:
    branches: [main]
    paths:
      - "adapter/**"

  schedule:
    # every midnight
    - cron: "0 0 * * *"

  workflow_dispatch:

jobs:
  run:
    runs-on: ubuntu-latest
    strategy:
      fail-fast: false
      matrix:
        profile:
          - postgres
          - bigquery
          - duckdb
          - snowflake
          - redshift
          # enable with next release of dbt-trino
          # - trino
        python: ["3.7", "3.8"]
        include:
          - profile: duckdb
            adapter_version: 1.2.2
          - profile: snowflake
            teleport: true

    concurrency:
      group: "${{ github.head_ref || github.run_id }}-${{ github.workflow }}-${{ matrix.profile }}-${{ matrix.python }}"
      cancel-in-progress: true

    steps:
      - uses: actions/checkout@v2
        with:
          path: 'fal'

      - name: Start Docker database
        working-directory: fal/integration_tests
        if: contains(fromJSON('["postgres"]'), matrix.profile)
        run: docker-compose up -d

      - name: Start trino
        working-directory: fal/adapter/integration_tests/configs/trino
        if: contains(fromJSON('["trino"]'), matrix.profile)
        run: docker-compose up -d

      - name: Setup latest dependencies
        working-directory: fal/adapter/integration_tests
        run: |
          python -m venv .venv
          source .venv/bin/activate
          pip install --upgrade pip
          pip install build wheel

          if [[ -z '${{ matrix.adapter_version }}' ]]
          then
            pip install 'dbt-${{ matrix.profile }}'
          else
            pip install 'dbt-${{ matrix.profile }}==${{ matrix.adapter_version }}'
          fi

          pushd ..
          if [[ '${{ matrix.teleport }}' == 'true' ]]
          then
            pip install -e ".[${{ matrix.profile }},teleport]"
          else
            pip install -e ".[${{ matrix.profile }}]"
          fi
          popd

      - name: Setup behave
        working-directory: fal/adapter/integration_tests
        run: pip install behave

      - name: Run tests
        id: test_run
        working-directory: fal/adapter/integration_tests
        env:
          FAL_STATS_ENABLED: false
          # BigQuery
          KEYFILE: ${{ secrets.GCP_SA_KEY }}
          GCLOUD_PROJECT: ${{ secrets.GCP_PROJECT_ID }}
          BQ_DATASET: ${{ secrets.BQ_DATASET }}
          # Snowflake
          SF_ACCOUNT: ${{ secrets.SF_ACCOUNT }}
          SF_USER: ${{ secrets.SF_USER }}
          SF_PASSWORD: ${{ secrets.SF_PASSWORD }}
          SF_ROLE: ${{ secrets.SF_ROLE }}
          SF_DATABASE: ${{ secrets.SF_DATABASE }}
          SF_WAREHOUSE: ${{ secrets.SF_WAREHOUSE }}
          SF_SCHEMA: ${{ secrets.SF_SCHEMA }}
          # Duckdb
          DB_PATH: ${{ github.workspace }}/duck.db
          # Redshift
          RS_HOST: ${{ secrets.RS_HOST }}
          RS_PORT: ${{ secrets.RS_PORT }}
          RS_USER: ${{ secrets.RS_USER }}
          RS_PASSWORD: ${{ secrets.RS_PASSWORD }}
          RS_DBNAME: ${{ secrets.RS_DBNAME }}
          RS_SCHEMA: ${{ secrets.RS_SCHEMA }}
          # Teleport
          AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
          AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
        run: |
          source .venv/bin/activate

          # Database and schema setup for sources
          if [[ '${{ matrix.profile }}' == "bigquery" ]]
          then
            export DBT_DATABASE="$GCLOUD_PROJECT" DBT_SCHEMA="$BQ_DATASET"
          fi
          if [[ '${{ matrix.profile }}' == "snowflake" ]]
          then
            export DBT_DATABASE="$SF_DATABASE" DBT_SCHEMA="$SF_SCHEMA"
          fi
          if [[ '${{ matrix.profile }}' == "duckdb" ]]
          then
            # TODO: which to use for sources? Example:
            #   database: "{{ env_var('DBT_DATABASE', 'test') }}"
            #   schema: "{{ env_var('DBT_SCHEMA', 'dbt_fal') }}"
            export DBT_DATABASE="" DBT_SCHEMA=""
          fi
          if [[ '${{ matrix.profile }}' == "redshift" ]]
          then
            export DBT_DATABASE="$RS_DBNAME" DBT_SCHEMA="$RS_SCHEMA"
          fi

          if [[ '${{ matrix.profile }}' == "bigquery" ]]
          then
            # Save the credentials to a keyfile
            echo $KEYFILE > $HOME/keyfile.json
            ls -la $HOME/keyfile.json
            export KEYFILE_DIR=$HOME
            echo 'keyfile is ready'
          fi

          # Could not get the real job_id easily from context
          UUID=$(uuidgen | head -c8)
          export DB_NAMESPACE="${{ github.run_id }}_${UUID}"

          BEHAVE_TAGS=""
          if [[ '${{ matrix.teleport }}' != 'true' ]]
          then
            BEHAVE_TAGS="--tags=-teleport"
          fi

          behave $BEHAVE_TAGS -fplain -D profile=${{ matrix.profile }}

      - name: Send custom JSON data to Slack workflow
        if: (failure() || cancelled()) && github.event_name == 'schedule'
        id: slack
        uses: slackapi/slack-github-action@v1.18.0
        with:
          # For posting a rich message using Block Kit
          payload: |
            {
              "text": "Integration tests failed for dbt-${{ matrix.profile }}@${{ matrix.dbt }} (Python ${{ matrix.python }})\nhttps://github.com/fal-ai/fal/actions/runs/${{ github.run_id }}"
            }
        env:
          SLACK_WEBHOOK_URL: ${{ secrets.SLACK_WEBHOOK_URL }}
          SLACK_WEBHOOK_TYPE: INCOMING_WEBHOOK
